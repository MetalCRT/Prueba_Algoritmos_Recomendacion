# üìä Prueba de Algoritmos de Recomendaci√≥n

Se utilizaron **LightGCN** y **BPR** sobre el dataset **MovieLens-100k**.

---

## üõ†Ô∏è Explicaci√≥n del procedimiento

1. Se instalaron los paquetes de Python necesarios:  
   - [`jupyter`](https://jupyter.org/)  
   - [`RecBole`](https://recbole.io/)  

2. Se crearon par√°metros de prueba en un archivo `test.yaml` para definir las m√©tricas de evaluaci√≥n.

3. Se ejecutaron las pruebas con el algoritmo **LightGCN**:
   - **Prueba 1:** M√©tricas *NDCG* y *Precision*.
   - **Prueba 2:** M√©trica *RMSE*.

4. Se repitieron las pruebas con **BPR** usando los mismos par√°metros para comparar resultados.

---

## üìö Explicaci√≥n e interpretaci√≥n

### üîπ Algoritmos
- **LightGCN**: Algoritmo de recomendaci√≥n basado en _embeddings_, es decir, vectores en una matriz. Su prop√≥sito es entrenar una funci√≥n de puntuaci√≥n, que le asigne mejores puntajes a pares de usuario-objeto que sean buenas recomendaciones, y peores puntajes en caso contrario.   
- **BPR** (*Bayesian Personalized Ranking*): Optimiza preferencias relativas entre pares de interacciones (i, j), aprendiendo que si un usuario prefiere un √≠tem *i* frente a otro *j*, se ajusten las preferencias del modelo para reflejarlo.

---

### üîπ M√©tricas de evaluaci√≥n
- **RMSE**: Mide el error medio al predecir calificaciones.  
  Ejemplo: Un RMSE de `0.5` significa un error promedio de media estrella (en escala 1‚Äì5). Menor valor = mejor resultado.
- **Precision**: Proporci√≥n de resultados relevantes sobre todos los resultados.
- **NDCG**: M√©trica que nos indica la comparaci√≥n del ranking de objetos, con el obtenido por el modelo (es decir, mientras m√°s cercano al 1 es esta m√©trica, m√°s cerca est√° la prueba del ranking ideal de las pel√≠culas del dataset).  

---

## üìà Resultados

- **LightGCN** tuvo mejor desempe√±o en RMSE (dado que mientras menor puntaje, menos error en esta m√©trica).  
- **BPR** tuvo mejor desempe√±o en m√©tricas de NDCG y Precision.

![Resultados](https://github.com/MetalCRT/Prueba_Algoritmos_Recomendacion/blob/main/Resultados.png)  

üìå **Interpretaci√≥n:**  
- Al ser un dataset relativamente peque√±o, se observa que **BPR** tiene mejor desempe√±o en las m√©tricas de _ranking_ (*Precision* y *NDCG*), es decir, el ordenamiento de las pel√≠culas por usuario. Por otro lado, **LightGCN** tiene mejor desempe√±o en *RMSE*, lo que indica que tiene menos error al predecir los *ratings* exactos de los objetos.
- Esto implica un **trade-off** al elegir algoritmo:
  - Si importa m√°s recomendar pel√≠culas adecuadas: **BPR**
  - Si importa m√°s predecir con exactitud la puntuaci√≥n: **LightGCN**
